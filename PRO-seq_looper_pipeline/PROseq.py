#!/usr/bin/env python
"""
PRO-seq pipeline - builds on Danko lab's PROseq mapper
"""

from argparse import ArgumentParser
import os
import sys
import subprocess
import re
import pypiper
import yaml
import pandas as pd
import numpy as np

###############
### Credits ###
###############

__author__ = "Martin Jaeger"
__copyright__ = "Copyright 2018, Martin Jaeger"
__credits__ = []
__license__ = "GPL3"
__version__ = "0.3"
__maintainer__ = "Martin Jaeger"
__email__ = "mjaeger@cemm.oeaw.ac.at"
__status__ = "development"


########################
### Argument Parsing ###
########################
parser = ArgumentParser(description='Pypiper arguments.')

parser = pypiper.add_pypiper_args(parser, groups=["pypiper", "resource", "config"])

parser.add_argument('-y', '--sample_yaml', dest='sample_config', help='yaml config file with sample attributes; this file will be generated by looper if submitting multiple jobs in parallel', type=str)
parser.add_argument('-i', '--input_file', dest='input_file', help='Path to input raw read BAM file(s). Space-separated paths will be merged before processing.', nargs='+', type=str) # this will be parsed as a list and later converted to a single filepath string (either after merging or on the single input file)
parser.add_argument('-n', '--sample_name', dest='sample_name', help='Name of the sample (STR); will be used for subdirectories & file names', type=str)
parser.add_argument('-O', '--output_parent', dest='output_parent', help='Path to the directory, where sample data will go', type=str)
parser.add_argument('-g', '--genome', dest='genome', help='Specify the sample\'s target genome. Currently only ["hg38", or "hg19"].', type=str) # this falsely points to the concatenated genome
parser.add_argument('-s', '--spike_in', dest='spike_in', help='Specify spike-in genome. Currently only ["dm6"]', type=str) # this falsely points to the concatenated genome
parser.add_argument('-r', '--rDNA', dest='rDNA', help='When this flag is set, the target genome includes a single copy of the human rDNA repeat locus U13369.', type=str)
parser.add_argument('-a', '--annotation', dest='annotation', help='Indicate the type of genome annotation source. Currently only option is ["refGene"]', type=str)

args = parser.parse_args()

print '\nInput arguments provided:', args, '\n\n'

###################################
### enforce complete user input ###
###################################

if not args.sample_config:
    parser.print_help()
    raise SystemExit()

##################
### Initialize ###
##################

outfolder = os.path.abspath(os.path.join(args.output_parent,args.sample_name))

# Start Pypiper object
# Best practice is to name the pipeline with the name of the script or put the name in the pipeline interface.
pm = pypiper.PipelineManager(name = 'PROseq', outfolder = outfolder, args = args)

# create NGSTk object
tk = pypiper.NGSTk(pm=pm)

#####################################
### merge input BAMs if necessary ###
#####################################

if len(args.input_file) > 1:
    pm.timestamp("Merging BAM files from replicates: ")

    sample_merged = True

    raw_folder = os.path.join(outfolder,'merged_raw')
    if not os.path.exists(raw_folder):
        os.mkdir(raw_folder)

    merged_path = os.path.join(raw_folder, '{0}.bam'.format(args.sample_name))

    cmd = tk.merge_bams(
        input_bams=args.input_file,  # this is a list of sample paths
        merged_bam=merged_path
    )
    pm.run(cmd, merged_path, shell=True)
    pm.clean_add(merged_path, conditional=True)
    pm.clean_add(merged_path.replace('.bam', '.bai'), conditional=True)

    args.input_file = merged_path

else:
    sample_merged = False
    args.input_file = args.input_file[0]  # args.input_file is now a single filepath string

##############
### FASTQC ###
##############
pm.timestamp('### FASTQC on raw BAM: ')

QC_folder = os.path.join(outfolder,'FASTQC')
if not os.path.exists(QC_folder):
    os.mkdir(QC_folder)

cmd = tk.fastqc_rename(
    input_bam=args.input_file,
    output_dir=QC_folder,
    sample_name=args.sample_name
)

pm.run(cmd, os.path.join(QC_folder, "{0}_fastqc.zip".format(args.sample_name)), shell=True, nofail=True)

#######################
### rawBAM to fastq ###
#######################
pm.timestamp('### Converting raw BAM to gzipped FASTQ: ')

fq_folder = os.path.join(outfolder, 'fastq')
if not os.path.exists(fq_folder):
    os.mkdir(fq_folder)

raw_fq_name = '{0}.fastq.gz'.format(args.sample_name)
raw_fq_path = os.path.join(fq_folder, raw_fq_name)

cmd = 'bamtools convert -format fastq -in {0} | gzip > {1}'.format(args.input_file, raw_fq_path)

pm.run(cmd, raw_fq_path)
pm.clean_add(raw_fq_path, conditional=True)

################
### Trimming ###
################
pm.timestamp('### Removing adapter sequences & converting to reverse complement: ')

clean_fq_name = 'finalreads_{0}.fastq.gz'.format(args.sample_name)
clean_fq_path = os.path.join(fq_folder, clean_fq_name)
adapters = pm.config.parameters.cutadapt.adapters.PROseq
min_length = pm.config.parameters.cutadapt.min_length
fastx_reverse_complement = pm.config.tools.fastx_reverse_complement

cmd = 'cutadapt -a {0} --minimum-length={1} {2} | {3} -z -o {4}'.format(adapters, min_length, raw_fq_path, fastx_reverse_complement, clean_fq_path)

pm.run(cmd, clean_fq_path)
pm.clean_add(clean_fq_path, conditional=True)

#########################
### Bowtie2 alignment ###
#########################
pm.timestamp('### Aligning with bowtie2: ')

aligned_folder = os.path.join(outfolder,'aligned')
if not os.path.exists(aligned_folder):
    os.mkdir(aligned_folder)

aligned_name = '{0}.sorted.bam'.format(args.sample_name)
aligned_path = os.path.join(aligned_folder, aligned_name)

# parse target genome from sample annotation sheet
if args.rDNA and args.spike_in:
    concat_genome = '{0}_{1}_rDNA'.format(args.genome, args.spike_in)
elif args.spike_in:
    concat_genome = '{0}_{1}'.format(args.genome, args.spike_in)
else:
    concat_genome = str(args.genome)

ref_index = pm.config.resources.genomes[concat_genome]

cmd = 'bowtie2 -p {0} -x {1} -U {2} --very-sensitive | samtools view -bS - | samtools sort -m {3}M -@ {0} -o {4}'.format(args.cores, ref_index, clean_fq_path, int(args.mem)/int(args.cores)*75/100, aligned_path) # manually limiting to 75% of mem/core due to samtools sort otherwise going over memory...
cmd2 = 'samtools index {0}'.format(aligned_path)

pm.run([cmd, cmd2], aligned_path)
pm.clean_add(aligned_path, conditional=True)
pm.clean_add('{0}.bai'.format(aligned_path), conditional=True)

#######################################
### count unfiltered spike-in reads ###
#######################################
# documenting spike-in stats before MAPQ filtering; not used for normalization!
if args.spike_in:
    pm.timestamp('### Counting number of unfiltered mapped total, {0} & {1} reads: '.format(args.spike_in, args.genome))

    norm_folder = os.path.join(outfolder, 'spike-in_normalization')
    if not os.path.exists(norm_folder):
        os.mkdir(norm_folder)

    norm_name = '{0}_readcount_{1}.txt'.format(args.spike_in, args.sample_name)
    norm_path = os.path.join(norm_folder, norm_name)

    cmd = 'samtools view {0} | awk \'BEGIN{{OFS="\\t"}} $3 ~ /{1}_/ {{++count}} END {{print "total_mapped","spike-in","human_reads","alpha""\\n"NR,count,NR-count,1e6/count}}\' > {2}'.format(aligned_path, args.spike_in, norm_path)

    pm.run(cmd, norm_path)
else:
    pm.timestamp('### No spike-in genome provided, proceeding with MAPQ filtering... ')

#################################
### MAPQ filter the alignment ###
#################################
pm.timestamp('### Filtering aligned reads with MAPQ{0}: '.format(pm.config.parameters.bowtie2.min_MAPQ))

filtered_name = '{0}_MAPQ{1}.sorted.bam'.format(args.sample_name, pm.config.parameters.bowtie2.min_MAPQ)
filtered_path = os.path.join(aligned_folder, filtered_name)

cmd = 'samtools view -b -q {0} {1} > {2}'.format(pm.config.parameters.bowtie2.min_MAPQ, aligned_path, filtered_path)
cmd2 = 'samtools index {0}'.format(filtered_path)

pm.run([cmd, cmd2], filtered_path)
#pm.clean_add(filtered_path, conditional=True)
#pm.clean_add('{0}.bai'.format(filtered_path), conditional=True)

##########################################
### count MAPQ-filtered spike-in reads ###
##########################################
if args.spike_in:
    pm.timestamp('### Counting number of MAPQ{0}-filtered total, {1} & {2} reads: '.format(pm.config.parameters.bowtie2.min_MAPQ, args.spike_in, args.genome))

    MAPQnorm_name = '{0}_readcount_MAPQ{1}_{2}.txt'.format(args.spike_in, pm.config.parameters.bowtie2.min_MAPQ, args.sample_name)
    MAPQnorm_path = os.path.join(norm_folder, MAPQnorm_name)

    cmd = 'samtools view {0} | awk \'BEGIN{{OFS="\\t"}} $3 ~ /{1}_/ {{++count}} END {{print "total_mapped","spike-in","human_reads","alpha""\\n"NR,count,NR-count,1e6/count}}\' > {2}'.format(filtered_path, args.spike_in, MAPQnorm_path)

    pm.run(cmd, MAPQnorm_path)

    spike_in_readcounts_df = pd.read_table(MAPQnorm_path)
    alpha = 1e6 / spike_in_readcounts_df.loc[0, 'spike-in'] #the 1 million constant is arbitrarily set to output reads-per-million-reference
    print "Spike-in normalization factor alpha={0:9.3f}, derived as 1e6/{1} (the number of MAPQ20 reads that align to {2}).".format(alpha, spike_in_readcounts_df.loc[0, 'spike-in'], args.spike_in)

elif args.rDNA:
    pm.timestamp('### No spike-in genome provided, proceeding with counting rDNA reads ... ')
else:
    pm.timestamp('### No spike-in genome provided, proceeding with 1bp BED files ... ')

######################################
### count MAPQ-filtered rDNA reads ###
######################################
if args.rDNA:
    pm.timestamp('### Counting number of MAPQ{0}-filtered total, rDNA & {1} reads: '.format(pm.config.parameters.bowtie2.min_MAPQ, args.genome))

    MAPQrDNA_name = 'rDNA_readcount_MAPQ{0}_{1}.txt'.format(pm.config.parameters.bowtie2.min_MAPQ, args.sample_name)
    MAPQrDNA_path = os.path.join(norm_folder, MAPQrDNA_name)

    cmd = 'samtools view {0} | awk \'BEGIN{{OFS="\\t"}} $3 ~ /rDNA_/ {{++count}} END {{print "total_mapped","rDNA","human_reads""\\n"NR,count,NR-count}}\' > {1}'.format(filtered_path, MAPQrDNA_path)

    pm.run(cmd, MAPQrDNA_path)
else:
    pm.timestamp('### No rDNA repeat locus included in alignment index, proceeding with 1bp BED files ... ')

#####################################
### trim reads to 1bp => BED file ###
#####################################
pm.timestamp('### Generating MAPQ{0} filtered 1bp BED file on {1} (3\' base is retained): '.format(pm.config.parameters.bowtie2.min_MAPQ, concat_genome))

bed_folder = os.path.join(outfolder, 'bed_1bp')
if not os.path.exists(bed_folder):
    os.mkdir(bed_folder)

bed1bp_name = '{0}_1bp.sorted.bed'.format(args.sample_name)
bed1bp_path = os.path.join(bed_folder, bed1bp_name)

concat_genome_sizes = pm.config.resources.chrom_sizes[concat_genome]

cmd = 'bedtools bamtobed -i {0} | awk \'BEGIN{{OFS="\\t"}} ($6 == "+") {{print $1,$3-1,$3,$4,$5,$6}}; ($6 == "-") {{print $1,$2,$2+1,$4,$5,$6}}\' | LC_COLLATE=C sort -k1,1 -k2,2n - > {1}'.format(filtered_path, bed1bp_path)

# this is another option to retain only args.genome reads
#cmd = 'bedtools bamtobed -i {0} | awk \'(NR==FNR) {{chr[$1]; next}}; ($1 in chr) {{print $0}}\' {1} - | awk \'BEGIN{{OFS="\\t"}} ($6 == "+") {{print $1,$3-1,$3,$4,$5,$6}}; ($6 == "-") {{print $1,$2,$2+1,$4,$5,$6}}\' | LC_COLLATE=C sort -k1,1 -k2,2n - > {2}'.format(filtered_path, pm.config.resources.chrom_sizes[args.genome], bed1bp_path)

pm.run(cmd, bed1bp_path)
pm.clean_add(bed1bp_path, conditional=True)

#############################
### 1bp BAM from BED file ###
#############################
pm.timestamp('### Generating {0} 1bp BAM from BED file (3\' base), indexing the BAM & gzipping BED: '.format(concat_genome))

bam1bp_name = '{0}_1bp.sorted.bam'.format(args.sample_name)
bam1bp_path = os.path.join(aligned_folder, bam1bp_name)

cmd = "bedToBam -i {0} -g {1} | samtools sort -m {2}M -@ {3} -o {4}".format(bed1bp_path, concat_genome_sizes, int(args.mem)/int(args.cores)*75/100, args.cores, bam1bp_path) # manually limiting to 75% of mem/core due to samtools sort otherwise going over memory...
cmd2 = 'samtools index {0}'.format(bam1bp_path)
cmd3 = 'gzip -c {0} > {0}.gz'.format(bed1bp_path)

# this is another option to retain only args.genome reads
#cmd = "awk '(NR==FNR) {{chr[$1]; next}}; ($1 in chr) {{print $0}}' {0} {1} | bedToBam -i - -g {0} | samtools sort -m {2}M -@ {3} -o {4}".format(pm.config.resources.chrom_sizes[args.genome], bed1bp_path, int(args.mem)/int(args.cores)*75/100, args.cores, bam1bp_path) # manually limiting to 75% of mem/core due to samtools sort otherwise going over memory...

pm.run([cmd, cmd2, cmd3], '{0}.gz'.format(bed1bp_path))

##################################
### generate bedGraph & bigWig ###
##################################
pm.timestamp('### Generating unnormalized bigWig files for {0} genome: '.format(concat_genome))

bw_folder = os.path.join(outfolder, 'bigWig')
if not os.path.exists(bw_folder):
    os.mkdir(bw_folder)

bg_plus_name = '{0}_plus_unnorm.bedGraph'.format(args.sample_name)
bg_plus_path = os.path.join(bw_folder, bg_plus_name)
bg_minus_name = '{0}_minus_unnorm.bedGraph'.format(args.sample_name)
bg_minus_path = os.path.join(bw_folder, bg_minus_name)

bw_plus_name = '{0}_plus_unnorm.bw'.format(args.sample_name)
bw_plus_path = os.path.join(bw_folder, bw_plus_name)
bw_minus_name = '{0}_minus_unnorm.bw'.format(args.sample_name)
bw_minus_path = os.path.join(bw_folder, bw_minus_name)

cmd = 'genomeCoverageBed -bg -strand + -ibam {0} -g {1} | LC_COLLATE=C sort -k1,1 -k2,2n - > {2}'.format(bam1bp_path, concat_genome_sizes, bg_plus_path)
cmd2 = 'genomeCoverageBed -bg -scale -1 -strand - -ibam {0} -g {1} | LC_COLLATE=C sort -k1,1 -k2,2n - > {2}'.format(bam1bp_path, concat_genome_sizes, bg_minus_path)
cmd3 = 'bedGraphToBigWig {0} {1} {2}'.format(bg_plus_path, concat_genome_sizes, bw_plus_path)
cmd4 = 'bedGraphToBigWig {0} {1} {2}'.format(bg_minus_path, concat_genome_sizes, bw_minus_path)

pm.run([cmd, cmd2, cmd3, cmd4], bw_minus_path)
pm.clean_add(bg_plus_path, conditional=True)
pm.clean_add(bg_minus_path, conditional=True)

# additionally generate spike-in normalized bedGraph and bigWig files, where --spike-in was used
if args.spike_in:
    pm.timestamp('### Generating {0} spike-in normalized bigWig files for the {1} genome with normalization factor alpha={2:9.3f}: '.format(args.spike_in, concat_genome, alpha))

    bg_plus_name = '{0}_plus_{1}_normalized.bedGraph'.format(args.sample_name, args.spike_in)
    bg_plus_path = os.path.join(bw_folder, bg_plus_name)
    bg_minus_name = '{0}_minus_{1}_normalized.bedGraph'.format(args.sample_name, args.spike_in)
    bg_minus_path = os.path.join(bw_folder, bg_minus_name)

    bw_plus_name = '{0}_plus_{1}_normalized.bw'.format(args.sample_name, args.spike_in)
    bw_plus_path = os.path.join(bw_folder, bw_plus_name)
    bw_minus_name = '{0}_minus_{1}_normalized.bw'.format(args.sample_name, args.spike_in)
    bw_minus_path = os.path.join(bw_folder, bw_minus_name)

    # using spike-in derived alpha as normalization factor
    cmd = 'genomeCoverageBed -bg -strand + -ibam {0} -g {1} -scale {2} | LC_COLLATE=C sort -k1,1 -k2,2n - > {3}'.format(bam1bp_path, concat_genome_sizes, alpha, bg_plus_path)
    cmd2 = 'genomeCoverageBed -bg -strand - -ibam {0} -g {1} -scale -{2} | LC_COLLATE=C sort -k1,1 -k2,2n - > {3}'.format(bam1bp_path, concat_genome_sizes, alpha, bg_minus_path)
    cmd3 = 'bedGraphToBigWig {0} {1} {2}'.format(bg_plus_path, concat_genome_sizes, bw_plus_path)
    cmd4 = 'bedGraphToBigWig {0} {1} {2}'.format(bg_minus_path, concat_genome_sizes, bw_minus_path)

    pm.run([cmd, cmd2, cmd3, cmd4], bw_minus_path)
    pm.clean_add(bg_plus_path, conditional=True)
    pm.clean_add(bg_minus_path, conditional=True)

#########################
### compute coverages ###
#########################
if args.annotation:
    pm.timestamp('### Computing unnormalized coverages on {0} annotations: '.format(pm.config.parameters.genome_annotations[args.annotation]))

    cov_folder = os.path.join(outfolder, '{0}_coverage'.format(args.annotation))
    if not os.path.exists(cov_folder):
        os.mkdir(cov_folder)

    full_length_name = '{0}_{1}_full_length_unnorm.cov'.format(args.sample_name, args.annotation)
    full_length_path = os.path.join(cov_folder, full_length_name)
    genebody_name = '{0}_{1}_genebody_unnorm.cov'.format(args.sample_name, args.annotation)
    genebody_path = os.path.join(cov_folder, genebody_name)
    TSS_name = '{0}_{1}_TSS+250_unnorm.cov'.format(args.sample_name, args.annotation)
    TSS_path = os.path.join(cov_folder, TSS_name)
    TTS_name = '{0}_{1}_TTS+2500_unnorm.cov'.format(args.sample_name, args.annotation)
    TTS_path = os.path.join(cov_folder, TTS_name)

    full_length_BED = pm.config.parameters.genome_annotations[args.annotation].full_length
    genebody_BED = pm.config.parameters.genome_annotations[args.annotation].genebody
    TSS_BED = pm.config.parameters.genome_annotations[args.annotation].TSS
    TTS_BED = pm.config.parameters.genome_annotations[args.annotation].TTS

    cmd = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts > {2}'.format(full_length_BED, bed1bp_path, full_length_path)
    cmd2 = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts > {2}'.format(genebody_BED, bed1bp_path, genebody_path)
    cmd3 = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts > {2}'.format(TSS_BED, bed1bp_path, TSS_path)
    cmd4 = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts > {2}'.format(TTS_BED, bed1bp_path, TTS_path)

    pm.run([cmd, cmd2, cmd3, cmd4], TTS_path)

    # using spike-in derived alpha scaling factor to generate normalized coverages
    if args.spike_in:
        pm.timestamp('### Computing {0} normalized coverages on {1} annotations: '.format(args.spike_in, pm.config.parameters.genome_annotations[args.annotation]))

        full_length_name = '{0}_{1}_full_length_{2}_normalized.cov'.format(args.sample_name, args.annotation, args.spike_in)
        full_length_path = os.path.join(cov_folder, full_length_name)
        genebody_name = '{0}_{1}_genebody_{2}_normalized.cov'.format(args.sample_name, args.annotation, args.spike_in)
        genebody_path = os.path.join(cov_folder, genebody_name)
        TSS_name = '{0}_{1}_TSS+250_{2}_normalized.cov'.format(args.sample_name, args.annotation, args.spike_in)
        TSS_path = os.path.join(cov_folder, TSS_name)
        TTS_name = '{0}_{1}_TTS+2500_{2}_normalized.cov'.format(args.sample_name, args.annotation, args.spike_in)
        TTS_path = os.path.join(cov_folder, TTS_name)

        cmd = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts | awk \'{{$7 = $7 * {2}; print}}\' - > {3}'.format(full_length_BED, bed1bp_path, alpha, full_length_path)
        cmd2 = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts | awk \'{{$7 = $7 * {2}; print}}\' - > {3}'.format(genebody_BED, bed1bp_path, alpha, genebody_path)
        cmd3 = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts | awk \'{{$7 = $7 * {2}; print}}\' - > {3}'.format(TSS_BED, bed1bp_path, alpha, TSS_path)
        cmd4 = 'coverageBed -a {0} -b {1}.gz -s -sorted -counts | awk \'{{$7 = $7 * {2}; print}}\' - > {3}'.format(TTS_BED, bed1bp_path, alpha, TTS_path)

        pm.run([cmd, cmd2, cmd3, cmd4], TTS_path)

###############
### cleanup ###
###############
# rm fastq/finalreads_$output_name.fastq.gz
# rm aligned/$output_name.sort.bam

print "Yeah! Finished analysis of PRO-seq sample {0}!".format(args.sample_name)

pm.stop_pipeline()
